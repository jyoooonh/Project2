{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../datasets/unlisted_filled_train_data.csv')\n",
    "test = pd.read_csv('../../datasets/unlisted_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_life_cycle = train[['도입기', '성장기', '성숙기', '쇠퇴기']]\n",
    "# test_life_cycle = test[['도입기', '성장기', '성숙기', '쇠퇴기']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['CASH FLOW 대 부채비율', '순운전자본비율', '자기자본구성비율', '경영자본순이익률', '총자본영업이익률',\n",
    "                      '금융비용부담률', '이윤분배율',\n",
    "                      '도입기', '성장기', '성숙기', '쇠퇴기']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[selected_features]\n",
    "x_test = test[selected_features]\n",
    "\n",
    "y_train = train['부실판단']\n",
    "y_test = test['부실판단']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.fillna(test['영업년수'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# labels = x_train.columns\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# x_train = pd.DataFrame(data=x_train, columns = labels)\n",
    "# x_test = pd.DataFrame(data=x_test, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_Accuracy_Scores: [0.66808914 0.81218876 0.8204885  0.83068532 0.67749585]\n",
      "CV_Precision_Scores: [0.64017398 0.80350554 0.77425903 0.78621256 0.6294504 ]\n",
      "CV_Recall_Scores: [0.7676624  0.82637571 0.90464896 0.90848743 0.86344239]\n",
      "CV_F1_Scores: [0.69814575 0.81478017 0.83439072 0.84293885 0.72810876]\n",
      "CV_ROC/AUC: [0.72037809 0.88867603 0.89643598 0.90325257 0.72292056]\n",
      "\n",
      "=======교차검증 결과=======\n",
      "CV_Accuracy_mean: 0.762\n",
      "CV_Precision_mean: 0.727\n",
      "CV_Recall_mean: 0.854\n",
      "CV_F1_스코어_mean: 0.784\n",
      "CV_ROC_AUC+스코어_mean: 0.826\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.723\n",
      "Precision: 0.611\n",
      "Recall: 0.679\n",
      "F1 스코어: 0.643\n",
      "ROC AUC 스코어: 0.714\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "logit_model = LogisticRegression()\n",
    "\n",
    "# Cross Validation\n",
    "cv_accuracy = cross_val_score(logit_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(logit_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(logit_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(logit_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(logit_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 결과=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "logit_model.fit(x_train, y_train)\n",
    "y_pred = logit_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 스코어: {f1:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.813\n",
      "Precision: 0.717\n",
      "Recall: 0.811\n",
      "F1 스코어: 0.761\n",
      "ROC AUC 스코어: 0.812\n"
     ]
    }
   ],
   "source": [
    "# Random Forest 모델 생성 및 학습\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=29, min_samples_split=7, min_samples_leaf=8, max_depth=7)\n",
    "\n",
    "# # Cross Validation\n",
    "# cv_accuracy = cross_val_score(rf_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "# cv_precision = cross_val_score(rf_model, x_train, y_train, cv=5, scoring='precision')\n",
    "# cv_recall = cross_val_score(rf_model, x_train, y_train, cv=5, scoring='recall')\n",
    "# cv_f1 = cross_val_score(rf_model, x_train, y_train, cv=5, scoring='f1')\n",
    "# cv_roc_auc = cross_val_score(rf_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "# print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "# print(\"CV_Precision_Scores:\", cv_precision)\n",
    "# print(\"CV_Recall_Scores:\", cv_recall)\n",
    "# print(\"CV_F1_Scores:\", cv_f1)\n",
    "# print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "# print('\\n=======교차검증 결과=======')\n",
    "# print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "# print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "# print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "# print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "# print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "\n",
    "rf_model.fit(x_train, y_train)\n",
    "y_pred_rf = rf_model.predict(x_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_rf:.3f}')\n",
    "print(f'Precision: {precision_rf:.3f}')\n",
    "print(f'Recall: {recall_rf:.3f}')\n",
    "print(f'F1 스코어: {f1_rf:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_rf:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.67828355 0.78871235 0.87455537 0.87953521 0.68626986]\n",
      "CV_Precision_Scores: [0.65027978 0.88007495 0.87328605 0.87670588 0.64534024]\n",
      "CV_Recall_Scores: [0.77145567 0.66840607 0.87618596 0.88335704 0.82740635]\n",
      "CV_F1_Scores: [0.70570375 0.75977352 0.8747336  0.88001889 0.72511947]\n",
      "CV_ROC/AUC: [0.75548076 0.92853716 0.93838382 0.94147811 0.74624002]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.781\n",
      "CV_Precision_mean: 0.785\n",
      "CV_Recall_mean: 0.805\n",
      "CV_F1_스코어_mean: 0.789\n",
      "CV_ROC_AUC+스코어_mean: 0.862\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.766\n",
      "Precision: 0.661\n",
      "Recall: 0.748\n",
      "F1 스코어: 0.702\n",
      "ROC AUC 스코어: 0.762\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost 모델 생성 및 학습\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "# Cross Validation\n",
    "cv_accuracy = cross_val_score(adaboost_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(adaboost_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(adaboost_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(adaboost_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(adaboost_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "adaboost_model.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_model.predict(x_test)\n",
    "\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "precision_adaboost = precision_score(y_test, y_pred_adaboost)\n",
    "recall_adaboost = recall_score(y_test, y_pred_adaboost)\n",
    "f1_adaboost = f1_score(y_test, y_pred_adaboost)\n",
    "roc_auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_adaboost:.3f}')\n",
    "print(f'Precision: {precision_adaboost:.3f}')\n",
    "print(f'Recall: {recall_adaboost:.3f}')\n",
    "print(f'F1 스코어: {f1_adaboost:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_adaboost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.68610716 0.85155324 0.91795115 0.92387954 0.69409533]\n",
      "CV_Precision_Scores: [0.66239139 0.84337349 0.89972777 0.89982111 0.65671642]\n",
      "CV_Recall_Scores: [0.75912755 0.86337761 0.94070209 0.95400664 0.81365576]\n",
      "CV_F1_Scores: [0.70746796 0.85325832 0.91975881 0.92612198 0.72681067]\n",
      "CV_ROC/AUC: [0.75807458 0.92837464 0.96831214 0.97135256 0.76047276]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.815\n",
      "CV_Precision_mean: 0.792\n",
      "CV_Recall_mean: 0.866\n",
      "CV_F1_스코어_mean: 0.827\n",
      "CV_ROC_AUC_mean: 0.877\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.804\n",
      "Precision: 0.705\n",
      "Recall: 0.807\n",
      "F1 스코어: 0.752\n",
      "ROC AUC 스코어: 0.805\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = XGBClassifier()\n",
    "\n",
    "# Cross Validation\n",
    "cv_accuracy = cross_val_score(xgboost_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(xgboost_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(xgboost_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(xgboost_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(xgboost_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "xgboost_model.fit(x_train, y_train)\n",
    "y_pred_xgboost = xgboost_model.predict(x_test)\n",
    "\n",
    "accuracy_xgboost = accuracy_score(y_test, y_pred_xgboost)\n",
    "precision_xgboost = precision_score(y_test, y_pred_xgboost)\n",
    "recall_xgboost = recall_score(y_test, y_pred_xgboost)\n",
    "f1_xgboost = f1_score(y_test, y_pred_xgboost)\n",
    "roc_auc_xgboost = roc_auc_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_xgboost:.3f}')\n",
    "print(f'Precision: {precision_xgboost:.3f}')\n",
    "print(f'Recall: {recall_xgboost:.3f}')\n",
    "print(f'F1 스코어: {f1_xgboost:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_xgboost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.67045993 0.78159829 0.90656865 0.9162912  0.67465023]\n",
      "CV_Precision_Scores: [0.66268382 0.83363581 0.90286251 0.8981184  0.65772107]\n",
      "CV_Recall_Scores: [0.69464201 0.71916509 0.91603416 0.92792793 0.70697013]\n",
      "CV_F1_Scores: [0.68041715 0.77602041 0.90094787 0.92056837 0.68493151]\n",
      "CV_ROC/AUC: [0.71054745 0.89681635 0.95356633 0.96432363 0.72546095]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.790\n",
      "CV_Precision_mean: 0.791\n",
      "CV_Recall_mean: 0.793\n",
      "CV_F1_스코어_mean: 0.793\n",
      "CV_ROC_AUC+스코어_mean: 0.850\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.813\n",
      "Precision: 0.725\n",
      "Recall: 0.794\n",
      "F1 스코어: 0.758\n",
      "ROC AUC 스코어: 0.809\n"
     ]
    }
   ],
   "source": [
    "# Bagging 모델 생성 및 학습\n",
    "bagging_model = BaggingClassifier()\n",
    "\n",
    "cv_accuracy = cross_val_score(bagging_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(bagging_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(bagging_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(bagging_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(bagging_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "bagging_model.fit(x_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(x_test)\n",
    "\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "precision_bagging = precision_score(y_test, y_pred_bagging)\n",
    "recall_bagging = recall_score(y_test, y_pred_bagging)\n",
    "f1_bagging = f1_score(y_test, y_pred_bagging)\n",
    "roc_auc_bagging = roc_auc_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_bagging:.3f}')\n",
    "print(f'Precision: {precision_bagging:.3f}')\n",
    "print(f'Recall: {recall_bagging:.3f}')\n",
    "print(f'F1 스코어: {f1_bagging:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_bagging:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.67069701 0.82689115 0.8204885  0.83329381 0.67773299]\n",
      "CV_Precision_Scores: [0.64117647 0.79145516 0.76438356 0.77963405 0.62860082]\n",
      "CV_Recall_Scores: [0.77524893 0.88757116 0.92647059 0.9293504  0.86913229]\n",
      "CV_F1_Scores: [0.70186735 0.83676208 0.83765816 0.84793424 0.72955224]\n",
      "CV_ROC/AUC: [0.71875641 0.89964015 0.90476412 0.91092908 0.7212916 ]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.766\n",
      "CV_Precision_mean: 0.721\n",
      "CV_Recall_mean: 0.878\n",
      "CV_F1_스코어_mean: 0.791\n",
      "CV_ROC_AUC+스코어_mean: 0.831\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.719\n",
      "Precision: 0.605\n",
      "Recall: 0.681\n",
      "F1 스코어: 0.641\n",
      "ROC AUC 스코어: 0.711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM 모델 생성 및 학습\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "cv_accuracy = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "svm_model.fit(x_train, y_train)\n",
    "y_pred_svm = svm_model.predict(x_test)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "roc_auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_svm:.3f}')\n",
    "print(f'Precision: {precision_svm:.3f}')\n",
    "print(f'Recall: {recall_svm:.3f}')\n",
    "print(f'F1 스코어: {f1_svm:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_svm:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.65457563 0.84870761 0.88617501 0.8928148  0.6718046 ]\n",
      "CV_Precision_Scores: [0.61760462 0.81276596 0.84579439 0.84913611 0.62007287]\n",
      "CV_Recall_Scores: [0.81175913 0.90607211 0.94449715 0.95542911 0.88762447]\n",
      "CV_F1_Scores: [0.7014956  0.8568865  0.89242492 0.89915216 0.7301092 ]\n",
      "CV_ROC/AUC: [0.71004373 0.91366561 0.93852699 0.94203931 0.73323778]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.791\n",
      "CV_Precision_mean: 0.749\n",
      "CV_Recall_mean: 0.901\n",
      "CV_F1_스코어_mean: 0.816\n",
      "CV_ROC_AUC+스코어_mean: 0.848\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.764\n",
      "Precision: 0.657\n",
      "Recall: 0.752\n",
      "F1 스코어: 0.702\n",
      "ROC AUC 스코어: 0.762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM 모델 생성 및 학습\n",
    "svm_model = SVC(kernel='rbf')\n",
    "\n",
    "cv_accuracy = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(svm_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "svm_model.fit(x_train, y_train)\n",
    "y_pred_svm = svm_model.predict(x_test)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "roc_auc_svm = roc_auc_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_svm:.3f}')\n",
    "print(f'Precision: {precision_svm:.3f}')\n",
    "print(f'Recall: {recall_svm:.3f}')\n",
    "print(f'F1 스코어: {f1_svm:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_svm:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16868, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16868, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16868, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16868, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16868, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8435, number of negative: 8434\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500030 -> initscore=0.000119\n",
      "[LightGBM] [Info] Start training from score 0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 8434, number of negative: 8435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794\n",
      "[LightGBM] [Info] Number of data points in the train set: 16869, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499970 -> initscore=-0.000119\n",
      "[LightGBM] [Info] Start training from score -0.000119\n",
      "=======교차검증 결과=======\n",
      "CV_Accuracy_Scores: [0.69487909 0.85511027 0.91463125 0.92198245 0.69172397]\n",
      "CV_Precision_Scores: [0.66599354 0.84927671 0.89511754 0.89982031 0.65421273]\n",
      "CV_Recall_Scores: [0.78188715 0.86337761 0.93927894 0.94973921 0.81365576]\n",
      "CV_F1_Scores: [0.71930207 0.85626911 0.91666667 0.92410611 0.72527473]\n",
      "CV_ROC/AUC: [0.77080828 0.93377494 0.96698841 0.97304304 0.76910264]\n",
      "\n",
      "=======교차검증 평균값=======\n",
      "CV_Accuracy_mean: 0.816\n",
      "CV_Precision_mean: 0.793\n",
      "CV_Recall_mean: 0.870\n",
      "CV_F1_스코어_mean: 0.828\n",
      "CV_ROC_AUC+스코어_mean: 0.883\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 10543, number of negative: 10543\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1797\n",
      "[LightGBM] [Info] Number of data points in the train set: 21086, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "=======Test 데이터 평가======\n",
      "Accuracy: 0.806\n",
      "Precision: 0.705\n",
      "Recall: 0.812\n",
      "F1 스코어: 0.755\n",
      "ROC AUC 스코어: 0.807\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# LGBM 모델 생성 및 학습\n",
    "lgbm_model = LGBMClassifier()\n",
    "\n",
    "cv_accuracy = cross_val_score(lgbm_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(lgbm_model, x_train, y_train, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(lgbm_model, x_train, y_train, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(lgbm_model, x_train, y_train, cv=5, scoring='f1')\n",
    "cv_roc_auc = cross_val_score(lgbm_model, x_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('=======교차검증 결과=======')\n",
    "print(\"CV_Accuracy_Scores:\", cv_accuracy)\n",
    "print(\"CV_Precision_Scores:\", cv_precision)\n",
    "print(\"CV_Recall_Scores:\", cv_recall)\n",
    "print(\"CV_F1_Scores:\", cv_f1)\n",
    "print(\"CV_ROC/AUC:\", cv_roc_auc)\n",
    "\n",
    "print('\\n=======교차검증 평균값=======')\n",
    "print(f'CV_Accuracy_mean: {cv_accuracy.mean():.3f}')\n",
    "print(f'CV_Precision_mean: {cv_precision.mean():.3f}')\n",
    "print(f'CV_Recall_mean: {cv_recall.mean():.3f}')\n",
    "print(f'CV_F1_스코어_mean: {cv_f1.mean():.3f}')\n",
    "print(f'CV_ROC_AUC+스코어_mean: {cv_roc_auc.mean():.3f}')\n",
    "\n",
    "lgbm_model.fit(x_train, y_train)\n",
    "y_pred_lgbm = lgbm_model.predict(x_test)\n",
    "\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "precision_lgbm = precision_score(y_test, y_pred_lgbm)\n",
    "recall_lgbm = recall_score(y_test, y_pred_lgbm)\n",
    "f1_lgbm = f1_score(y_test, y_pred_lgbm)\n",
    "roc_auc_lgbm = roc_auc_score(y_test, y_pred_lgbm)\n",
    "\n",
    "print(f'\\n=======Test 데이터 평가======')\n",
    "print(f'Accuracy: {accuracy_lgbm:.3f}')\n",
    "print(f'Precision: {precision_lgbm:.3f}')\n",
    "print(f'Recall: {recall_lgbm:.3f}')\n",
    "print(f'F1 스코어: {f1_lgbm:.3f}')\n",
    "print(f'ROC AUC 스코어: {roc_auc_lgbm:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(256, input_dim = 12, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(x_train, y_train, epochs=200, batch_size=5)\n",
    "\n",
    "# print(\"\\n Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
    "# y_pred = model.predict(x_test)\n",
    "# y_pred = binarize(y_pred, threshold=0.5)  # 예측값을 0.5 임계값을 기준으로 이진 분류로 변환\n",
    "\n",
    "# # 정확도(accuracy) 계산\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # F1 점수(f1 score) 계산\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# # 재현율(recall) 계산\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# print(f\"recall: {recall:.4f}\")\n",
    "\n",
    "# # 정밀도(precision) 계산\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# print(f\"precision: {precision:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
